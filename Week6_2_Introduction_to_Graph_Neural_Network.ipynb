{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpxXLPN6wKYH4RhnRs17gN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JLee823/2023-1st-AI-assisted-drug-discovery-SNU/blob/main/Week6_2_Introduction_to_Graph_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Graph Neural Network \n",
        "------\n",
        "\n",
        "최근 분자를 다루는데 있어서 그래프 신경망이 큰 인기를 얻고 있다. 그 이유는 분자는 그 자체로 그래프로 나타내는 것이 너무나 자연스럽기 때문이다. 또한 그래프 신경망 자체도 매우 복잡한 구조를 가진 데이터, 예를 들어 SNS데이터, 지식 데이터 등 에서 특성을 학습시키고 정보를 추출하는데 좋은 방법이기 때문이다. "
      ],
      "metadata": {
        "id": "ydxgqk511ocd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 그래프란?\n",
        "-----\n",
        "수학적으로 그래프는 노드(node, vertex)와 링크(edge, line)로 이루어진 추상적인 연결 상태, $G(V,E)$를 의미한다. 각 노드는 노드의 특징을 수치적으로 나타내는 node feature vector를 가지고 있으며, edge도 edge feature를 가질 수 있다. \n",
        "\n",
        "노드 사이의 연결상태는 adjacency matrix, $A$로 정의된다. 만일 node $i$, $j$가 연결되어 있으면 adjacency matrix의 성분 $A_{ij} = 1$로 정의되고 그렇지 않으면 $A_{ij}=0$으로 정의된다. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kAVj0etl2l_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://mathworld.wolfram.com/images/eps-svg/AdjacencyMatrix_1002.svg\" width=600>"
      ],
      "metadata": {
        "id": "-kDTpm7y32JM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "자기 자신에서 출발하여 도착하는 self-loop을 허용하면 adjacency matrix의 diagonal 성분이 0이 아닌 값을 가진다. "
      ],
      "metadata": {
        "id": "Bl3qkekH35BC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/28/6n-graph2.svg/400px-6n-graph2.svg.png\" width=300> <img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/a773011024de5e3cbe8da03e97c79e1fe3101937\" width=300>"
      ],
      "metadata": {
        "id": "tdtd6H694I1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 예제에서 (1,1) 성분이 2인 이유는 self-loop의 경우, 출발과 도착이 동일하기 때문에 2번 고려되기 때문이다. "
      ],
      "metadata": {
        "id": "cgXLFm8q4Yt6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Directed graph\n",
        "-----\n",
        "위의 예제의 그래프는 방향이 정의되지 않은 그래프이나, 각 edge는 방향을 가질 수 있다. \n",
        "일반적으로 노드 $i$에서 시작해서 $j$에 도착하는 경우 $A_{ij} = 1$로 정의된다. "
      ],
      "metadata": {
        "id": "S5e1Qvxn4hkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/86/Symmetric_group_4%3B_Cayley_graph_4%2C9%3B_numbers.svg/1920px-Symmetric_group_4%3B_Cayley_graph_4%2C9%3B_numbers.svg.png\" width=300> <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/22/Symmetric_group_4%3B_Cayley_graph_4%2C9_%28adjacency_matrix%29.svg/1920px-Symmetric_group_4%3B_Cayley_graph_4%2C9_%28adjacency_matrix%29.svg.png\" width=300>"
      ],
      "metadata": {
        "id": "9zGWhPFC4qAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weighted graph\n",
        "-----\n",
        "또한, 그래프에서 각 edge는 edge의 weight를 가질 수 있다.\n",
        "\n",
        "<img src=\"https://www.cs.mtsu.edu/~xyang/3080/images/adjMatrixWeightedGraph.jpeg\" width=500>\n"
      ],
      "metadata": {
        "id": "evyAr1Dy5F6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph Convolutions\n",
        "------\n",
        "아래와 같은 **방향이 없는** 예시 그래프가 있을 때, 꼭지점(vertex)의 집합은 $V=\\{1,2,3,4\\}$로 정의된다. 그리고 edge의 집합은 $E=\\{(1,2), (2,3), (3,4), (2,4)\\}$로 정의된다. 방향이 없기 때문에 $(2,1)$은 $E$에 들어가지 않는다. 이를 바탕으로 그래프 $\\mathscr{G} = (E,V)$로 정의된다. \n",
        "\n",
        "<img src=\"https://uvadlc-notebooks.readthedocs.io/en/latest/_images/example_graph.svg\" width=500>\n"
      ],
      "metadata": {
        "id": "dcsALwgt6RY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 그래프의 adjacency matrix는 다음과 같이 정의된다. 이와 같은 adjacency matrix는 구현이 편하다는 장점이 있으나 노드 개수 $N$의 제곱에 비례하므로 아주 큰 그래프에서는 메모리를 많이 사용하게 된다. 그러므로 실제 구현에서는 edge list가 더 많이 사용된다. \n",
        "\n",
        "\\begin{split}A = \\begin{bmatrix}\n",
        "    0 & 1 & 0 & 0\\\\\n",
        "    1 & 0 & 1 & 1\\\\\n",
        "    0 & 1 & 0 & 1\\\\\n",
        "    0 & 1 & 1 & 0\n",
        "\\end{bmatrix}\\end{split}"
      ],
      "metadata": {
        "id": "amtZ9Xl07s_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 그래프를 기반으로 하여 Graph Convolution에 대해서 간단히 알아보자. Graph Convolution은 크게 3가지의 단계로 이루어진다. \n",
        "\n",
        "1. feature transformation: node feature\b에 대한 convolution(matrix multiplication)을 통한 feature 변형을 통한 message 생성\n",
        "2. message passing: 연결된 node들, neighbor들,의 feature를 받아오거나 전달함. \n",
        "3. message merging: 메세지를 취합하여 새로운 feature를 생성하는 step\n",
        "\n",
        "<img src=\"https://uvadlc-notebooks.readthedocs.io/en/latest/_images/graph_message_passing.svg\" width=600>"
      ],
      "metadata": {
        "id": "XAAItZy08G9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이해를 돕기 위해서 아래의 아주 간단한 graph를 생각해보자. "
      ],
      "metadata": {
        "id": "4XnC3rey-13z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://miro.medium.com/v2/resize:fit:1070/format:webp/1*ivtGccpkSXEDVwf9mgyaNg.png\" width=600>"
      ],
      "metadata": {
        "id": "uKEFr-Yr-xPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$W$ 행렬이 곱해지는 부분을 생략하고, adjacency matrix와 feature matrix $X$를 곱해보자."
      ],
      "metadata": {
        "id": "EpARSBn1_Ppj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PM16z40n5wcmwDGuMI6PHA.png\" width=600>"
      ],
      "metadata": {
        "id": "NjtRepRE_KRY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bj4uMijrEOdKRu011MmA5w.png\" width=600>"
      ],
      "metadata": {
        "id": "tx9bh_AU_Yas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과적으로 얻어진 matrix는 각 노드의 자기 자신의 feature와 neighboring node들의 feature를 합한 결과가 된다. 실제 graph convolution에서는 feature matrix에 weight matrix $W$가 한 번 곱해지고 adjacency matrix가 곱해진다. "
      ],
      "metadata": {
        "id": "Q0nPYbFG_bi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 과정을 수학적으로 표현하면 아래와 같다. \n",
        "\n",
        "$H^{(l+1)} = \\sigma\\left(\\hat{D}^{-1/2}\\hat{A}\\hat{D}^{-1/2}H^{(l)}W^{(l)}\\right)$\n",
        "\n",
        "위 식에서 $H$는 각 node들이 가진 feature들을 하나로 모은 matrix이고, **$W$는 feature를 변형시키는 matrix이다. 이 $W$는 모든 node들이 공유한다.** 일반적으로 Graph convolution에서는 adjacency matrix에 identity matrix를 더하여 자기 자신에 대한 정보도 고려한다, $A=A+I$.  $D$는 diagonal matrix이고 $D_{ii}$ 값은 노드 $i$가 몇 개의 neighbor를 가지는지 표현한다. \n",
        "위의 식에서 $\\hat{D}^{-1/2}$가 곱해지는 이유는 이웃한 노드가 많은 노드의 경우, 그 값이 계속해서 커지는 문제가 발생할 수 있기 때문에 이를 정규화 해주기 위한 것이다. \n",
        "\n",
        "위 식에서 $\\sigma$는 임의의 활성 함수이다. 일반적으로 sigmoid, ReLU, ELU등이 많이 사용된다. "
      ],
      "metadata": {
        "id": "iLBrhmNQ9uGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example"
      ],
      "metadata": {
        "id": "atxY2VuBHEg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 process를 간단히 pytorch로 구현하면 다음과 같다. "
      ],
      "metadata": {
        "id": "husoMvpwA_Z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "X18wwQISBNm7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, c_in, c_out):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(c_in, c_out)\n",
        "\n",
        "    def forward(self, node_feats, adj_matrix):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            node_feats - Tensor with node features of shape [batch_size, num_nodes, c_in]\n",
        "            adj_matrix - Batch of adjacency matrices of the graph. If there is an edge from i to j, adj_matrix[b,i,j]=1 else 0.\n",
        "                         Supports directed edges by non-symmetric matrices. Assumes to already have added the identity connections.\n",
        "                         Shape: [batch_size, num_nodes, num_nodes]\n",
        "        \"\"\"\n",
        "        # Num neighbours = number of incoming edges\n",
        "        # dim=-1 means sum over the last dimension, along the column, here.\n",
        "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
        "\n",
        "        # Transforming an input feature\n",
        "        node_feats = self.projection(node_feats)\n",
        "\n",
        "        # Performing batch matrix multiplication\n",
        "        node_feats = torch.bmm(adj_matrix, node_feats)\n",
        "\n",
        "        # Normalize feature vector with the number of neighbors\n",
        "        node_feats = node_feats / num_neighbours\n",
        "        return node_feats"
      ],
      "metadata": {
        "id": "-16c7r8kBHEN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "연습을 위한 input feature와 adjacency matrix를 아래와 같이 정의하자. "
      ],
      "metadata": {
        "id": "hHANZFfXG-Hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "node_feats = torch.arange(8, dtype=torch.float32).view(1, 4, 2)\n",
        "adj_matrix = torch.Tensor([[[1, 1, 0, 0],\n",
        "                            [1, 1, 1, 1],\n",
        "                            [0, 1, 1, 1],\n",
        "                            [0, 1, 1, 1]]])\n",
        "\n",
        "print(\"Node features:\\n\", node_feats)\n",
        "print(\"\\nAdjacency matrix:\\n\", adj_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk-_4rUICLl_",
        "outputId": "154ea2b2-a5cf-4515-d7d4-a9543b7d0332"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node features:\n",
            " tensor([[[0., 1.],\n",
            "         [2., 3.],\n",
            "         [4., 5.],\n",
            "         [6., 7.]]])\n",
            "\n",
            "Adjacency matrix:\n",
            " tensor([[[1., 1., 0., 0.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 input feature와 adjacency matrix를 이용해서 GCNlayer에 통과시켜보자. 예시를 위해서 $W$ 행렬을 항등행렬로 정의하자. "
      ],
      "metadata": {
        "id": "Da_HHDoRHHn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer = GCNLayer(c_in=2, c_out=2)\n",
        "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
        "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
        "\n",
        "with torch.no_grad():\n",
        "    out_feats = layer(node_feats, adj_matrix)\n",
        "\n",
        "print(\"Adjacency matrix\", adj_matrix)\n",
        "print(\"Input features\", node_feats)\n",
        "print(\"Output features\", out_feats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAFD6vuvCNIM",
        "outputId": "becd9726-206c-4dd1-adde-f2cbf52d941b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjacency matrix tensor([[[1., 1., 0., 0.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.]]])\n",
            "Input features tensor([[[0., 1.],\n",
            "         [2., 3.],\n",
            "         [4., 5.],\n",
            "         [6., 7.]]])\n",
            "Output features tensor([[[1., 2.],\n",
            "         [3., 4.],\n",
            "         [4., 5.],\n",
            "         [4., 5.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "계산 결과, 연결된 node들의 feature의 평균 값이 리턴 됨을 알 수 있다. \n",
        "\n",
        "다양한 Graph neural network 모델들은 이와 같이 message passing을 하는 방식과 그 message들을 취합 하는 방식에서 차이를 가지고 있다. "
      ],
      "metadata": {
        "id": "rEYVP7QIDGqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Molecule as a graph\n",
        "------\n",
        "위와 같은 그래프의 정의를 바탕으로 분자를 어떻게 그래프로 나타낼 수 있을지 생각해보자. 사실, 분자는 그 형태 자체로 직관적으로 그래프로 나타내는 것이 자연스럽다는 것을 알 수 있다. 가장 자연스러운 변환은 각 원자가 하나의 노드가 되고 공유 결합을 연결로 표현하는 것이다. 그리고 필요에 따라서는 추가적으로 artificial bond를 추가할 수도 있다. \n"
      ],
      "metadata": {
        "id": "GafLKn4i5jPQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www.researchgate.net/publication/330845050/figure/fig1/AS:722433858363394@1549253062903/Modelling-a-molecule-as-a-graph-Individual-atoms-and-ring-structures-are-mapped-to.png\" width=600>"
      ],
      "metadata": {
        "id": "UqB-aeye2eg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래 예제에서는 원자에 따라서 3차원 입력 tensor 값이 1 또는 0으로 주어져있다. \n",
        "\n"
      ],
      "metadata": {
        "id": "lu3ns5x0JCRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://dmol.pub/_images/gnn_11_0.png\" width=600>\n",
        "\n",
        "credit: https://dmol.pub/dl/gnn.html"
      ],
      "metadata": {
        "id": "STpGkF3lAP87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래 애니메이션에서는 convolution이 일어나면 각 feature 값이 어떻게 변화하는지 알 수 있다. "
      ],
      "metadata": {
        "id": "Z-aDlhHjJtZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://dmol.pub/_images/gcn.gif\" width=600>"
      ],
      "metadata": {
        "id": "Tkd8SsHiAlnb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What can graph neural network do? \n",
        "-------\n",
        "\n",
        "그래프 신경망을 이용하여 크게 다음의 3가지 task를 수행할 수 있다. \n",
        "\n",
        "1. node-level prediction: 각 노드의 성질을 예측할 수 있다. 주변의 연결 상태로 부터 특정 노드의 성질을 예측하는 task.\n",
        "2. edge-level prediction: 각 edge의 성질 혹은 임의의 두 노드 사이의 연결의 존재 여부 등을 예측할 수 있다. 예를 들어 netflix에서 다음 영상을 추천하는 것도 이에 해당한다고 할 수 있다. \n",
        "3. graph-level prediction: 전체 그래프의 특정 성질을 예측할 수 있다. 예를 들어 분자의 solubility, toxicity prediction등이 이에 해당한다. "
      ],
      "metadata": {
        "id": "e6Zx2iT3Hxf1"
      }
    }
  ]
}